{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#from lime import lime_text\n",
    "#from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "import textwrap\n",
    "import spacy\n",
    "\n",
    "import pickle\n",
    "\n",
    "import lftk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modele\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "#from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'myfunctions' from '/Users/andrzejrostkowski/python-code/PRACA_LICENCJACKA/FINAL/myfunctions.py'>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import myfunctions\n",
    "\n",
    "importlib.reload(myfunctions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myfunctions import evaluate_model, tfidf_vectorize, spacy_embedding, extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definiuj listę modeli, które chcesz wczytać\n",
    "models_tfidf = [\n",
    "    'Logistic Regression',\n",
    "    'K-Nearest Neighbors',\n",
    "    'Random Forest',\n",
    "    'Multinomial Naive Bayes',\n",
    "    'SVM',\n",
    "    'MLP',\n",
    "]\n",
    "\n",
    "models_spacy = [\n",
    "    'Logistic Regression',\n",
    "    'K-Nearest Neighbors',\n",
    "    'Random Forest',\n",
    "    'SVM',\n",
    "    'MLP',\n",
    "]\n",
    "\n",
    "models_linguistic = [\n",
    "    'Logistic Regression',\n",
    "    'K-Nearest Neighbors',\n",
    "    'Random Forest',\n",
    "    'SVM',\n",
    "    'MLP',\n",
    "]\n",
    "\n",
    "\n",
    "# Stwórz pusty słownik do przechowywania wczytanych modeli\n",
    "loaded_models_tfidf = {}\n",
    "loaded_models_spacy = {}\n",
    "loaded_models_linguistic = {}\n",
    "\n",
    "for name in models_tfidf:\n",
    "    # Wczytaj model z pliku\n",
    "    with open(f'MODELE/{name}_model_tfidf.pkl', 'rb') as f:\n",
    "        loaded_models_tfidf[name] = pickle.load(f)\n",
    "\n",
    "for name in models_spacy:\n",
    "    # Wczytaj model z pliku\n",
    "    with open(f'MODELE/{name}_model_spacy.pkl', 'rb') as f:\n",
    "        loaded_models_spacy[name] = pickle.load(f)\n",
    "\n",
    "for name in models_linguistic:\n",
    "    # Wczytaj model z pliku\n",
    "    with open(f'MODELE/{name}_model_linguistic.pkl', 'rb') as f:\n",
    "        loaded_models_linguistic[name] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LogisticRegression())]),\n",
       " 'K-Nearest Neighbors': Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', KNeighborsClassifier())]),\n",
       " 'Random Forest': Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                 ('clf', RandomForestClassifier())]),\n",
       " 'Multinomial Naive Bayes': Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())]),\n",
       " 'SVM': Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC())]),\n",
       " 'MLP': Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                 ('clf', MLPClassifier(max_iter=1000))])}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_models_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zbiory danych do ewaluacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrzejrostkowski/python-code/.venv/lib/python3.9/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('df_preprocessed_v1.csv')\n",
    "\n",
    "amazon_single = pd.read_csv(\"DANE/reviews_generated1_cut.csv\")\n",
    "yelp_single = pd.read_csv(\"DANE/reviews_generated2_cut.csv\")\n",
    "amazon_ext = pd.read_csv(\"DANE/fake-reviews-dataset.csv\")\n",
    "yelp_ext = pd.read_csv(\"DANE/combat-ai-restaurants-test.csv\")\n",
    "general_gpt3 = pd.read_csv(\"DANE/reviews_generated4_cut.csv\")\n",
    "general_gpt4 = pd.read_csv(\"DANE/reviews_generated5_cut.csv\")\n",
    "llama3 = pd.read_csv(\"DANE/reviews_generated6_cut.csv\")\n",
    "\n",
    "human = pd.read_excel(\"DANE/restaurant_reviews_anonymized.xlsx\")\n",
    "# with open(\"DANE/restaurant_reviews_anonymized.csv\", 'rb') as f:\n",
    "#   human = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [amazon_single,yelp_single,general_gpt3,general_gpt4,llama3]:\n",
    "    df.drop(\"Unnamed: 0\",axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_single = amazon_single.rename(columns={\"text_\": \"text\"})\n",
    "amazon_ext = amazon_ext.rename(columns={\"text_\": \"text\"})\n",
    "amazon_ext['target'] = amazon_ext['label'].map({'CG': 1, 'OR': 0})\n",
    "yelp_ext = yelp_ext.rename(columns={\"label\": \"target\"})\n",
    "\n",
    "human = human.loc[:,['Review','Real']]\n",
    "human.columns = [\"text\",\"target\"]\n",
    "human['target'] = 1 - human['target']\n",
    "\n",
    "amazon_single = amazon_single[amazon_single.apply(lambda x: len(x['text']) >= 20, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  target\n",
      "0     I tried the \"famous\" Mac and Cheese burger at ...       1\n",
      "1     The Arlo hotel in Soho is great for budget-min...       1\n",
      "2     I thought the lunch specials at this restauran...       1\n",
      "3     I had a great experience at Covid this morning...       1\n",
      "4     Da Toscano is one of the more unique Italian r...       0\n",
      "...                                                 ...     ...\n",
      "4758  It says the lunch special is offered every day...       0\n",
      "4759  I stopped by this restaurant for a quick pre-d...       1\n",
      "4760  I was excited to try this restaurant after rea...       1\n",
      "4761  I had a great time at this restaurant! The ser...       1\n",
      "4762  i've eaten at this restaurant, as well as orde...       0\n",
      "\n",
      "[4763 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(yelp_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8765\n",
      "10000\n",
      "40432\n",
      "4763\n",
      "2000\n",
      "1000\n",
      "110\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "for name,df in zbiory.items():\n",
    "    print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "zbiory = {\"amazon_single\": amazon_single, \"yelp_single\": yelp_single, \"amazon_ext\": amazon_ext, \"yelp_ext\": yelp_ext, \"general_gpt3\": general_gpt3, \"general_gpt4\": general_gpt4, \"human\": human, \"llama3\": llama3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET amazon_single...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   72.710\n",
      "Precision:   97.301\n",
      "Recall:   49.370\n",
      "F1 score:   65.503\n",
      "(0.7270964061608671, 0.9730077120822622, 0.49369565217391304, 0.6550331698875108)\n",
      "Evaluating K-Nearest Neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8s/bq8p3kcj0n7ckt9bpcm56l8r0000gn/T/ipykernel_25813/1068718450.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:   76.326\n",
      "Precision:   83.024\n",
      "Recall:   69.000\n",
      "F1 score:   75.365\n",
      "(0.7632629777524245, 0.8302380329584096, 0.69, 0.7536507182713997)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   69.846\n",
      "Precision:   94.579\n",
      "Recall:   45.130\n",
      "F1 score:   61.104\n",
      "(0.6984597832287507, 0.9457858769931663, 0.45130434782608697, 0.611037527593819)\n",
      "Evaluating Multinomial Naive Bayes...\n",
      "Accuracy:   79.738\n",
      "Precision:   95.285\n",
      "Recall:   64.587\n",
      "F1 score:   76.989\n",
      "(0.7973759269823161, 0.9528543938422065, 0.6458695652173913, 0.7698885721689557)\n",
      "Evaluating SVM...\n",
      "Accuracy:   75.836\n",
      "Precision:   96.410\n",
      "Recall:   56.043\n",
      "F1 score:   70.883\n",
      "(0.7583571021106674, 0.9640987284966342, 0.5604347826086956, 0.7088259554577949)\n",
      "Evaluating MLP...\n",
      "Accuracy:   79.772\n",
      "Precision:   94.689\n",
      "Recall:   65.109\n",
      "F1 score:   77.161\n",
      "(0.797718197375927, 0.9468858678469807, 0.6510869565217391, 0.7716089140796084)\n",
      "==================================================\n",
      "DATASET yelp_single...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   97.970\n",
      "Precision:   98.036\n",
      "Recall:   97.816\n",
      "F1 score:   97.926\n",
      "(0.9797, 0.9803640826344856, 0.9781632653061224, 0.9792624374297682)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   84.140\n",
      "Precision:   76.343\n",
      "Recall:   98.000\n",
      "F1 score:   85.827\n",
      "(0.8414, 0.7634340222575516, 0.98, 0.858266309204647)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   96.920\n",
      "Precision:   98.013\n",
      "Recall:   95.653\n",
      "F1 score:   96.819\n",
      "(0.9692, 0.9801338352153911, 0.956530612244898, 0.9681883908283413)\n",
      "Evaluating Multinomial Naive Bayes...\n",
      "Accuracy:   97.300\n",
      "Precision:   96.805\n",
      "Recall:   97.714\n",
      "F1 score:   97.258\n",
      "(0.973, 0.9680549939344926, 0.9771428571428571, 0.9725776965265083)\n",
      "Evaluating SVM...\n",
      "Accuracy:   98.110\n",
      "Precision:   98.022\n",
      "Recall:   98.122\n",
      "F1 score:   98.072\n",
      "(0.9811, 0.9802242609582059, 0.9812244897959184, 0.9807241203467618)\n",
      "Evaluating MLP...\n",
      "Accuracy:   97.800\n",
      "Precision:   97.292\n",
      "Recall:   98.245\n",
      "F1 score:   97.766\n",
      "(0.978, 0.9729183508488278, 0.9824489795918367, 0.9776604386677498)\n",
      "==================================================\n",
      "DATASET amazon_ext...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   50.020\n",
      "Precision:   51.053\n",
      "Recall:   0.960\n",
      "F1 score:   1.884\n",
      "(0.5001978630787495, 0.5105263157894737, 0.00959635931935101, 0.018838609438725964)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   50.477\n",
      "Precision:   51.637\n",
      "Recall:   15.057\n",
      "F1 score:   23.316\n",
      "(0.5047734467748318, 0.5163698049194232, 0.15057380292837358, 0.23315843897208074)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   50.022\n",
      "Precision:   50.744\n",
      "Recall:   1.519\n",
      "F1 score:   2.949\n",
      "(0.5002225959635932, 0.5074380165289256, 0.015185991294024535, 0.029489457758993323)\n",
      "Evaluating Multinomial Naive Bayes...\n",
      "Accuracy:   50.017\n",
      "Precision:   50.805\n",
      "Recall:   1.093\n",
      "F1 score:   2.140\n",
      "(0.5001731301939059, 0.5080459770114942, 0.01093193510091017, 0.021403321873032784)\n",
      "Evaluating SVM...\n",
      "Accuracy:   49.901\n",
      "Precision:   45.475\n",
      "Recall:   0.994\n",
      "F1 score:   1.946\n",
      "(0.4990106846062525, 0.45475113122171945, 0.009942619707162644, 0.019459773453383677)\n",
      "Evaluating MLP...\n",
      "Accuracy:   49.891\n",
      "Precision:   46.405\n",
      "Recall:   1.405\n",
      "F1 score:   2.727\n",
      "(0.49891175306687774, 0.46405228758169936, 0.01404827859121488, 0.027270981371231035)\n",
      "==================================================\n",
      "DATASET yelp_ext...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   58.598\n",
      "Precision:   95.260\n",
      "Recall:   17.783\n",
      "F1 score:   29.972\n",
      "(0.5859752256980895, 0.9525959367945824, 0.1778339654445849, 0.2997159090909091)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   67.793\n",
      "Precision:   74.347\n",
      "Recall:   53.982\n",
      "F1 score:   62.549\n",
      "(0.6779340751627125, 0.7434706906558328, 0.5398230088495575, 0.62548828125)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   59.878\n",
      "Precision:   95.833\n",
      "Recall:   20.354\n",
      "F1 score:   33.577\n",
      "(0.5987822800755827, 0.9583333333333334, 0.20353982300884957, 0.3357664233576642)\n",
      "Evaluating Multinomial Naive Bayes...\n",
      "Accuracy:   58.765\n",
      "Precision:   93.790\n",
      "Recall:   18.458\n",
      "F1 score:   30.845\n",
      "(0.587654839386941, 0.9379014989293362, 0.1845764854614412, 0.3084507042253521)\n",
      "Evaluating SVM...\n",
      "Accuracy:   57.653\n",
      "Precision:   95.408\n",
      "Recall:   15.761\n",
      "F1 score:   27.052\n",
      "(0.5765273986982994, 0.9540816326530612, 0.15760640539401602, 0.2705244122965642)\n",
      "Evaluating MLP...\n",
      "Accuracy:   57.842\n",
      "Precision:   95.285\n",
      "Recall:   16.182\n",
      "F1 score:   27.666\n",
      "(0.5784169640982574, 0.9528535980148883, 0.16182048040455121, 0.276657060518732)\n",
      "==================================================\n",
      "DATASET general_gpt3...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   99.100\n",
      "Precision:   99.298\n",
      "Recall:   98.901\n",
      "F1 score:   99.099\n",
      "(0.991, 0.9929789368104313, 0.989010989010989, 0.990990990990991)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   90.550\n",
      "Precision:   84.175\n",
      "Recall:   99.900\n",
      "F1 score:   91.366\n",
      "(0.9055, 0.8417508417508418, 0.999000999000999, 0.9136592051164916)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   98.750\n",
      "Precision:   98.509\n",
      "Recall:   99.001\n",
      "F1 score:   98.754\n",
      "(0.9875, 0.9850894632206759, 0.99000999000999, 0.9875435974090683)\n",
      "Evaluating Multinomial Naive Bayes...\n",
      "Accuracy:   98.850\n",
      "Precision:   98.512\n",
      "Recall:   99.201\n",
      "F1 score:   98.855\n",
      "(0.9885, 0.9851190476190477, 0.9920079920079921, 0.9885515181682429)\n",
      "Evaluating SVM...\n",
      "Accuracy:   98.700\n",
      "Precision:   99.094\n",
      "Recall:   98.302\n",
      "F1 score:   98.696\n",
      "(0.987, 0.9909365558912386, 0.983016983016983, 0.9869608826479438)\n",
      "Evaluating MLP...\n",
      "Accuracy:   98.050\n",
      "Precision:   98.390\n",
      "Recall:   97.702\n",
      "F1 score:   98.045\n",
      "(0.9805, 0.9839034205231388, 0.977022977022977, 0.9804511278195489)\n",
      "==================================================\n",
      "DATASET general_gpt4...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   97.800\n",
      "Precision:   98.580\n",
      "Recall:   97.006\n",
      "F1 score:   97.787\n",
      "(0.978, 0.9858012170385395, 0.9700598802395209, 0.9778672032193159)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   88.900\n",
      "Precision:   82.609\n",
      "Recall:   98.603\n",
      "F1 score:   89.900\n",
      "(0.889, 0.8260869565217391, 0.9860279441117764, 0.8989990900818926)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   98.000\n",
      "Precision:   98.390\n",
      "Recall:   97.605\n",
      "F1 score:   97.996\n",
      "(0.98, 0.9839034205231388, 0.9760479041916168, 0.9799599198396793)\n",
      "Evaluating Multinomial Naive Bayes...\n",
      "Accuracy:   97.500\n",
      "Precision:   96.667\n",
      "Recall:   98.403\n",
      "F1 score:   97.527\n",
      "(0.975, 0.9666666666666667, 0.9840319361277445, 0.9752720079129574)\n",
      "Evaluating SVM...\n",
      "Accuracy:   96.800\n",
      "Precision:   98.551\n",
      "Recall:   95.010\n",
      "F1 score:   96.748\n",
      "(0.968, 0.9855072463768116, 0.9500998003992016, 0.967479674796748)\n",
      "Evaluating MLP...\n",
      "Accuracy:   96.100\n",
      "Precision:   98.326\n",
      "Recall:   93.812\n",
      "F1 score:   96.016\n",
      "(0.961, 0.9832635983263598, 0.93812375249501, 0.9601634320735445)\n",
      "==================================================\n",
      "DATASET human...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   50.909\n",
      "Precision:   100.000\n",
      "Recall:   1.818\n",
      "F1 score:   3.571\n",
      "(0.509090909090909, 1.0, 0.01818181818181818, 0.03571428571428571)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   49.091\n",
      "Precision:   48.276\n",
      "Recall:   25.455\n",
      "F1 score:   33.333\n",
      "(0.4909090909090909, 0.4827586206896552, 0.2545454545454545, 0.3333333333333333)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   49.091\n",
      "Precision:   0.000\n",
      "Recall:   0.000\n",
      "F1 score:   0.000\n",
      "(0.4909090909090909, 0.0, 0.0, 0.0)\n",
      "Evaluating Multinomial Naive Bayes...\n",
      "Accuracy:   50.000\n",
      "Precision:   50.000\n",
      "Recall:   3.636\n",
      "F1 score:   6.780\n",
      "(0.5, 0.5, 0.03636363636363636, 0.06779661016949153)\n",
      "Evaluating SVM...\n",
      "Accuracy:   50.909\n",
      "Precision:   100.000\n",
      "Recall:   1.818\n",
      "F1 score:   3.571\n",
      "(0.509090909090909, 1.0, 0.01818181818181818, 0.03571428571428571)\n",
      "Evaluating MLP...\n",
      "Accuracy:   50.000\n",
      "Precision:   50.000\n",
      "Recall:   1.818\n",
      "F1 score:   3.509\n",
      "(0.5, 0.5, 0.01818181818181818, 0.03508771929824561)\n",
      "==================================================\n",
      "DATASET llama3...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   98.500\n",
      "Precision:   98.016\n",
      "Recall:   98.998\n",
      "F1 score:   98.504\n",
      "(0.985, 0.9801587301587301, 0.9899799599198397, 0.9850448654037887)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   89.400\n",
      "Precision:   82.587\n",
      "Recall:   99.800\n",
      "F1 score:   90.381\n",
      "(0.894, 0.8258706467661692, 0.9979959919839679, 0.9038112522686026)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   98.000\n",
      "Precision:   98.778\n",
      "Recall:   97.194\n",
      "F1 score:   97.980\n",
      "(0.98, 0.9877800407331976, 0.9719438877755511, 0.9797979797979798)\n",
      "Evaluating Multinomial Naive Bayes...\n",
      "Accuracy:   98.100\n",
      "Precision:   97.619\n",
      "Recall:   98.597\n",
      "F1 score:   98.106\n",
      "(0.981, 0.9761904761904762, 0.9859719438877755, 0.9810568295114656)\n",
      "Evaluating SVM...\n",
      "Accuracy:   98.400\n",
      "Precision:   98.397\n",
      "Recall:   98.397\n",
      "F1 score:   98.397\n",
      "(0.984, 0.9839679358717435, 0.9839679358717435, 0.9839679358717435)\n",
      "Evaluating MLP...\n",
      "Accuracy:   98.100\n",
      "Precision:   98.000\n",
      "Recall:   98.196\n",
      "F1 score:   98.098\n",
      "(0.981, 0.98, 0.9819639278557114, 0.980980980980981)\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Dataset', 'Model', 'Accuracy TFIDF'])\n",
    "\n",
    "for name, df in zbiory.items():\n",
    "    print(\"=\" * 50)  # Dodajemy linie złożoną z 50 znaków \"=\"\n",
    "    print((f\"DATASET {name}...\"))\n",
    "    for model in models_tfidf:\n",
    "        print(f\"Evaluating {model}...\")\n",
    "        metrics = evaluate_model(loaded_models_tfidf[model], df[\"text\"], df[\"target\"], cm=False)\n",
    "        print(metrics)\n",
    "        new_row = pd.DataFrame({'Dataset': [name], 'Model': [model], 'Accuracy TFIDF': [metrics[0]]})\n",
    "        results = pd.concat([results, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.727096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.763263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.698460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.797376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.758357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.797718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.979700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.841400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.969200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.981100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.500198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.504773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.500223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.500173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.499011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.498912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.585975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.677934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.598782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.587655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.576527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.578417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.905500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.980500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.889000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>human</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.509091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>human</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.490909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>human</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.490909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>human</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>human</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.509091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>human</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>llama3</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>llama3</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>llama3</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>llama3</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>llama3</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>llama3</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.981000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Dataset                    Model  Accuracy TFIDF\n",
       "0   amazon_single      Logistic Regression        0.727096\n",
       "1   amazon_single      K-Nearest Neighbors        0.763263\n",
       "2   amazon_single            Random Forest        0.698460\n",
       "3   amazon_single  Multinomial Naive Bayes        0.797376\n",
       "4   amazon_single                      SVM        0.758357\n",
       "5   amazon_single                      MLP        0.797718\n",
       "6     yelp_single      Logistic Regression        0.979700\n",
       "7     yelp_single      K-Nearest Neighbors        0.841400\n",
       "8     yelp_single            Random Forest        0.969200\n",
       "9     yelp_single  Multinomial Naive Bayes        0.973000\n",
       "10    yelp_single                      SVM        0.981100\n",
       "11    yelp_single                      MLP        0.978000\n",
       "12     amazon_ext      Logistic Regression        0.500198\n",
       "13     amazon_ext      K-Nearest Neighbors        0.504773\n",
       "14     amazon_ext            Random Forest        0.500223\n",
       "15     amazon_ext  Multinomial Naive Bayes        0.500173\n",
       "16     amazon_ext                      SVM        0.499011\n",
       "17     amazon_ext                      MLP        0.498912\n",
       "18       yelp_ext      Logistic Regression        0.585975\n",
       "19       yelp_ext      K-Nearest Neighbors        0.677934\n",
       "20       yelp_ext            Random Forest        0.598782\n",
       "21       yelp_ext  Multinomial Naive Bayes        0.587655\n",
       "22       yelp_ext                      SVM        0.576527\n",
       "23       yelp_ext                      MLP        0.578417\n",
       "24   general_gpt3      Logistic Regression        0.991000\n",
       "25   general_gpt3      K-Nearest Neighbors        0.905500\n",
       "26   general_gpt3            Random Forest        0.987500\n",
       "27   general_gpt3  Multinomial Naive Bayes        0.988500\n",
       "28   general_gpt3                      SVM        0.987000\n",
       "29   general_gpt3                      MLP        0.980500\n",
       "30   general_gpt4      Logistic Regression        0.978000\n",
       "31   general_gpt4      K-Nearest Neighbors        0.889000\n",
       "32   general_gpt4            Random Forest        0.980000\n",
       "33   general_gpt4  Multinomial Naive Bayes        0.975000\n",
       "34   general_gpt4                      SVM        0.968000\n",
       "35   general_gpt4                      MLP        0.961000\n",
       "36          human      Logistic Regression        0.509091\n",
       "37          human      K-Nearest Neighbors        0.490909\n",
       "38          human            Random Forest        0.490909\n",
       "39          human  Multinomial Naive Bayes        0.500000\n",
       "40          human                      SVM        0.509091\n",
       "41          human                      MLP        0.500000\n",
       "42         llama3      Logistic Regression        0.985000\n",
       "43         llama3      K-Nearest Neighbors        0.894000\n",
       "44         llama3            Random Forest        0.980000\n",
       "45         llama3  Multinomial Naive Bayes        0.981000\n",
       "46         llama3                      SVM        0.984000\n",
       "47         llama3                      MLP        0.981000"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrzejrostkowski/python-code/.venv/lib/python3.9/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0.0 and may not be 100% compatible with the current version (3.7.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Załaduj model spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrzejrostkowski/python-code/.venv/lib/python3.9/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0.0 and may not be 100% compatible with the current version (3.7.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "embedded_dfs = {}\n",
    "\n",
    "for name, df in zbiory.items():\n",
    "    frame = df['text'].to_frame()\n",
    "    frame.columns = ['text']\n",
    "    embedded = spacy_embedding(frame, \"text\", [], nlp)\n",
    "    embedded_dfs[f'{name}_embedded'] = embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedded_dfs.pickle', 'wb') as handle:\n",
    "    pickle.dump(embedded_dfs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET amazon_single...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   76.497\n",
      "Precision:   83.669\n",
      "Recall:   68.609\n",
      "F1 score:   75.394\n",
      "(0.7649743297204792, 0.8366914103923648, 0.6860869565217391, 0.7539417104634496)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   69.846\n",
      "Precision:   79.079\n",
      "Recall:   57.848\n",
      "F1 score:   66.817\n",
      "(0.6984597832287507, 0.7907875185735512, 0.5784782608695652, 0.6681732580037665)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   70.690\n",
      "Precision:   85.998\n",
      "Recall:   52.739\n",
      "F1 score:   65.382\n",
      "(0.7069024529378208, 0.8599787309464729, 0.5273913043478261, 0.6538202398598572)\n",
      "Evaluating SVM...\n",
      "Accuracy:   76.212\n",
      "Precision:   82.841\n",
      "Recall:   68.957\n",
      "F1 score:   75.264\n",
      "(0.7621220764403879, 0.8284147296944372, 0.6895652173913044, 0.7526396962866295)\n",
      "Evaluating MLP...\n",
      "Accuracy:   74.672\n",
      "Precision:   83.019\n",
      "Recall:   65.043\n",
      "F1 score:   72.940\n",
      "(0.7467199087278951, 0.8301886792452831, 0.6504347826086957, 0.7294002925402243)\n",
      "==================================================\n",
      "DATASET yelp_single...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   92.390\n",
      "Precision:   92.697\n",
      "Recall:   91.694\n",
      "F1 score:   92.192\n",
      "(0.9239, 0.9269651330720033, 0.9169387755102041, 0.9219246947778804)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   89.950\n",
      "Precision:   89.383\n",
      "Recall:   90.204\n",
      "F1 score:   89.792\n",
      "(0.8995, 0.8938321536905965, 0.9020408163265307, 0.8979177247333672)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   90.930\n",
      "Precision:   92.005\n",
      "Recall:   89.245\n",
      "F1 score:   90.604\n",
      "(0.9093, 0.920050494424574, 0.8924489795918368, 0.906039573189682)\n",
      "Evaluating SVM...\n",
      "Accuracy:   92.370\n",
      "Precision:   92.960\n",
      "Recall:   91.347\n",
      "F1 score:   92.146\n",
      "(0.9237, 0.929595015576324, 0.913469387755102, 0.921461657231086)\n",
      "Evaluating MLP...\n",
      "Accuracy:   93.570\n",
      "Precision:   94.353\n",
      "Recall:   92.408\n",
      "F1 score:   93.370\n",
      "(0.9357, 0.9435299020629297, 0.9240816326530612, 0.933704505619136)\n",
      "==================================================\n",
      "DATASET amazon_ext...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   48.363\n",
      "Precision:   42.467\n",
      "Recall:   9.230\n",
      "F1 score:   15.165\n",
      "(0.4836268302334784, 0.4246700045516614, 0.09230312623664425, 0.15164567249085736)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   48.830\n",
      "Precision:   45.912\n",
      "Recall:   13.138\n",
      "F1 score:   20.430\n",
      "(0.4883013454689355, 0.45911840968020745, 0.13138108428967155, 0.2042998346217453)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   49.493\n",
      "Precision:   46.925\n",
      "Recall:   7.736\n",
      "F1 score:   13.283\n",
      "(0.4949297586070439, 0.46924692469246926, 0.07736446379105659, 0.13282941950825938)\n",
      "Evaluating SVM...\n",
      "Accuracy:   48.239\n",
      "Precision:   41.960\n",
      "Recall:   9.191\n",
      "F1 score:   15.079\n",
      "(0.482390185991294, 0.4196025293586269, 0.09190740007914523, 0.15078720986852784)\n",
      "Evaluating MLP...\n",
      "Accuracy:   48.496\n",
      "Precision:   40.476\n",
      "Recall:   6.391\n",
      "F1 score:   11.039\n",
      "(0.4849624060150376, 0.40476190476190477, 0.06390977443609022, 0.11038961038961038)\n",
      "==================================================\n",
      "DATASET yelp_ext...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   62.461\n",
      "Precision:   82.755\n",
      "Recall:   31.142\n",
      "F1 score:   45.254\n",
      "(0.6246063405416754, 0.8275475923852184, 0.31142014327855033, 0.45254133496631965)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   60.487\n",
      "Precision:   76.089\n",
      "Recall:   30.173\n",
      "F1 score:   43.211\n",
      "(0.6048708796976695, 0.7608926673751328, 0.30172777075431945, 0.432106216053108)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   60.151\n",
      "Precision:   78.308\n",
      "Recall:   27.686\n",
      "F1 score:   40.909\n",
      "(0.6015116523199664, 0.7830750893921334, 0.27686472819216185, 0.4090909090909091)\n",
      "Evaluating SVM...\n",
      "Accuracy:   62.671\n",
      "Precision:   83.240\n",
      "Recall:   31.395\n",
      "F1 score:   45.594\n",
      "(0.6267058576527399, 0.8324022346368715, 0.3139485882848715, 0.4559363525091799)\n",
      "Evaluating MLP...\n",
      "Accuracy:   60.403\n",
      "Precision:   84.539\n",
      "Recall:   25.116\n",
      "F1 score:   38.726\n",
      "(0.6040310728532438, 0.8453900709219858, 0.25115887062789716, 0.387264457439896)\n",
      "==================================================\n",
      "DATASET general_gpt3...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   92.250\n",
      "Precision:   94.062\n",
      "Recall:   90.210\n",
      "F1 score:   92.096\n",
      "(0.9225, 0.940625, 0.9020979020979021, 0.9209586945436002)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   91.800\n",
      "Precision:   90.592\n",
      "Recall:   93.307\n",
      "F1 score:   91.929\n",
      "(0.918, 0.9059165858389913, 0.9330669330669331, 0.9192913385826772)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   93.550\n",
      "Precision:   93.863\n",
      "Recall:   93.207\n",
      "F1 score:   93.534\n",
      "(0.9355, 0.9386317907444668, 0.932067932067932, 0.9353383458646617)\n",
      "Evaluating SVM...\n",
      "Accuracy:   91.750\n",
      "Precision:   94.000\n",
      "Recall:   89.211\n",
      "F1 score:   91.543\n",
      "(0.9175, 0.94, 0.8921078921078921, 0.9154279856483855)\n",
      "Evaluating MLP...\n",
      "Accuracy:   93.750\n",
      "Precision:   95.342\n",
      "Recall:   92.008\n",
      "F1 score:   93.645\n",
      "(0.9375, 0.953416149068323, 0.9200799200799201, 0.936451448906965)\n",
      "==================================================\n",
      "DATASET general_gpt4...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   92.700\n",
      "Precision:   92.292\n",
      "Recall:   93.214\n",
      "F1 score:   92.751\n",
      "(0.927, 0.9229249011857708, 0.9321357285429142, 0.9275074478649454)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   90.200\n",
      "Precision:   88.528\n",
      "Recall:   92.415\n",
      "F1 score:   90.430\n",
      "(0.902, 0.8852772466539197, 0.9241516966067864, 0.904296875)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   92.300\n",
      "Precision:   92.063\n",
      "Recall:   92.615\n",
      "F1 score:   92.338\n",
      "(0.923, 0.9206349206349206, 0.9261477045908184, 0.9233830845771144)\n",
      "Evaluating SVM...\n",
      "Accuracy:   92.200\n",
      "Precision:   92.385\n",
      "Recall:   92.016\n",
      "F1 score:   92.200\n",
      "(0.922, 0.9238476953907816, 0.9201596806387226, 0.922)\n",
      "Evaluating MLP...\n",
      "Accuracy:   92.600\n",
      "Precision:   93.306\n",
      "Recall:   91.816\n",
      "F1 score:   92.555\n",
      "(0.926, 0.9330628803245437, 0.9181636726546906, 0.9255533199195171)\n",
      "==================================================\n",
      "DATASET human...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   55.455\n",
      "Precision:   87.500\n",
      "Recall:   12.727\n",
      "F1 score:   22.222\n",
      "(0.5545454545454546, 0.875, 0.12727272727272726, 0.2222222222222222)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   52.727\n",
      "Precision:   61.538\n",
      "Recall:   14.545\n",
      "F1 score:   23.529\n",
      "(0.5272727272727272, 0.6153846153846154, 0.14545454545454545, 0.23529411764705882)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   48.182\n",
      "Precision:   25.000\n",
      "Recall:   1.818\n",
      "F1 score:   3.390\n",
      "(0.4818181818181818, 0.25, 0.01818181818181818, 0.03389830508474576)\n",
      "Evaluating SVM...\n",
      "Accuracy:   55.455\n",
      "Precision:   87.500\n",
      "Recall:   12.727\n",
      "F1 score:   22.222\n",
      "(0.5545454545454546, 0.875, 0.12727272727272726, 0.2222222222222222)\n",
      "Evaluating MLP...\n",
      "Accuracy:   51.818\n",
      "Precision:   60.000\n",
      "Recall:   10.909\n",
      "F1 score:   18.462\n",
      "(0.5181818181818182, 0.6, 0.10909090909090909, 0.18461538461538463)\n",
      "==================================================\n",
      "DATASET llama3...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   92.400\n",
      "Precision:   93.429\n",
      "Recall:   91.182\n",
      "F1 score:   92.292\n",
      "(0.924, 0.9342915811088296, 0.9118236472945892, 0.922920892494929)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   87.400\n",
      "Precision:   88.454\n",
      "Recall:   85.972\n",
      "F1 score:   87.195\n",
      "(0.874, 0.8845360824742268, 0.8597194388777555, 0.8719512195121951)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   90.300\n",
      "Precision:   92.585\n",
      "Recall:   87.575\n",
      "F1 score:   90.010\n",
      "(0.903, 0.9258474576271186, 0.875751503006012, 0.9001029866117405)\n",
      "Evaluating SVM...\n",
      "Accuracy:   92.900\n",
      "Precision:   93.673\n",
      "Recall:   91.984\n",
      "F1 score:   92.821\n",
      "(0.929, 0.936734693877551, 0.9198396793587175, 0.9282103134479271)\n",
      "Evaluating MLP...\n",
      "Accuracy:   94.000\n",
      "Precision:   95.445\n",
      "Recall:   92.385\n",
      "F1 score:   93.890\n",
      "(0.94, 0.9544513457556936, 0.9238476953907816, 0.9389002036659878)\n"
     ]
    }
   ],
   "source": [
    "# Dodajemy nową kolumnę 'F1 Score Spacy' z domyślną wartością NaN\n",
    "results['Accuracy Spacy'] = np.nan\n",
    "\n",
    "for name, df in zbiory.items():\n",
    "    print(\"=\" * 50)  # Dodajemy linie złożoną z 50 znaków \"=\"\n",
    "    print((f\"DATASET {name}...\"))\n",
    "    for model in models_spacy:\n",
    "        print(f\"Evaluating {model}...\")\n",
    "        metrics = evaluate_model(loaded_models_spacy[model], embedded_dfs[f'{name}_embedded'], df[\"target\"], cm=False)\n",
    "        print(metrics)\n",
    "        # Dodajemy wyniki do nowej kolumny 'Accuracy Spacy'\n",
    "        results.loc[(results['Dataset'] == name) & (results['Model'] == model), 'Accuracy Spacy'] = metrics[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy TFIDF</th>\n",
       "      <th>Accuracy Spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.727096</td>\n",
       "      <td>0.764974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.763263</td>\n",
       "      <td>0.698460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.698460</td>\n",
       "      <td>0.706902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.797376</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.758357</td>\n",
       "      <td>0.762122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.797718</td>\n",
       "      <td>0.746720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.979700</td>\n",
       "      <td>0.923900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.841400</td>\n",
       "      <td>0.899500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.969200</td>\n",
       "      <td>0.909300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.973000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.981100</td>\n",
       "      <td>0.923700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.978000</td>\n",
       "      <td>0.935700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.500198</td>\n",
       "      <td>0.483627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.504773</td>\n",
       "      <td>0.488301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.500223</td>\n",
       "      <td>0.494930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.500173</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.499011</td>\n",
       "      <td>0.482390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.498912</td>\n",
       "      <td>0.484962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.585975</td>\n",
       "      <td>0.624606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.677934</td>\n",
       "      <td>0.604871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.598782</td>\n",
       "      <td>0.601512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.587655</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.576527</td>\n",
       "      <td>0.626706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.578417</td>\n",
       "      <td>0.604031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.991000</td>\n",
       "      <td>0.922500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.905500</td>\n",
       "      <td>0.918000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>0.935500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.988500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.987000</td>\n",
       "      <td>0.917500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.980500</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.978000</td>\n",
       "      <td>0.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.889000</td>\n",
       "      <td>0.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.923000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.968000</td>\n",
       "      <td>0.922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.961000</td>\n",
       "      <td>0.926000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>human</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.554545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>human</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.527273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>human</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.481818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>human</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>human</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.554545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>human</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.518182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>llama3</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>llama3</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>llama3</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>llama3</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>llama3</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.984000</td>\n",
       "      <td>0.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>llama3</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Dataset                    Model  Accuracy TFIDF  Accuracy Spacy\n",
       "0   amazon_single      Logistic Regression        0.727096        0.764974\n",
       "1   amazon_single      K-Nearest Neighbors        0.763263        0.698460\n",
       "2   amazon_single            Random Forest        0.698460        0.706902\n",
       "3   amazon_single  Multinomial Naive Bayes        0.797376             NaN\n",
       "4   amazon_single                      SVM        0.758357        0.762122\n",
       "5   amazon_single                      MLP        0.797718        0.746720\n",
       "6     yelp_single      Logistic Regression        0.979700        0.923900\n",
       "7     yelp_single      K-Nearest Neighbors        0.841400        0.899500\n",
       "8     yelp_single            Random Forest        0.969200        0.909300\n",
       "9     yelp_single  Multinomial Naive Bayes        0.973000             NaN\n",
       "10    yelp_single                      SVM        0.981100        0.923700\n",
       "11    yelp_single                      MLP        0.978000        0.935700\n",
       "12     amazon_ext      Logistic Regression        0.500198        0.483627\n",
       "13     amazon_ext      K-Nearest Neighbors        0.504773        0.488301\n",
       "14     amazon_ext            Random Forest        0.500223        0.494930\n",
       "15     amazon_ext  Multinomial Naive Bayes        0.500173             NaN\n",
       "16     amazon_ext                      SVM        0.499011        0.482390\n",
       "17     amazon_ext                      MLP        0.498912        0.484962\n",
       "18       yelp_ext      Logistic Regression        0.585975        0.624606\n",
       "19       yelp_ext      K-Nearest Neighbors        0.677934        0.604871\n",
       "20       yelp_ext            Random Forest        0.598782        0.601512\n",
       "21       yelp_ext  Multinomial Naive Bayes        0.587655             NaN\n",
       "22       yelp_ext                      SVM        0.576527        0.626706\n",
       "23       yelp_ext                      MLP        0.578417        0.604031\n",
       "24   general_gpt3      Logistic Regression        0.991000        0.922500\n",
       "25   general_gpt3      K-Nearest Neighbors        0.905500        0.918000\n",
       "26   general_gpt3            Random Forest        0.987500        0.935500\n",
       "27   general_gpt3  Multinomial Naive Bayes        0.988500             NaN\n",
       "28   general_gpt3                      SVM        0.987000        0.917500\n",
       "29   general_gpt3                      MLP        0.980500        0.937500\n",
       "30   general_gpt4      Logistic Regression        0.978000        0.927000\n",
       "31   general_gpt4      K-Nearest Neighbors        0.889000        0.902000\n",
       "32   general_gpt4            Random Forest        0.980000        0.923000\n",
       "33   general_gpt4  Multinomial Naive Bayes        0.975000             NaN\n",
       "34   general_gpt4                      SVM        0.968000        0.922000\n",
       "35   general_gpt4                      MLP        0.961000        0.926000\n",
       "36          human      Logistic Regression        0.509091        0.554545\n",
       "37          human      K-Nearest Neighbors        0.490909        0.527273\n",
       "38          human            Random Forest        0.490909        0.481818\n",
       "39          human  Multinomial Naive Bayes        0.500000             NaN\n",
       "40          human                      SVM        0.509091        0.554545\n",
       "41          human                      MLP        0.500000        0.518182\n",
       "42         llama3      Logistic Regression        0.985000        0.924000\n",
       "43         llama3      K-Nearest Neighbors        0.894000        0.874000\n",
       "44         llama3            Random Forest        0.980000        0.903000\n",
       "45         llama3  Multinomial Naive Bayes        0.981000             NaN\n",
       "46         llama3                      SVM        0.984000        0.929000\n",
       "47         llama3                      MLP        0.981000        0.940000"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## roBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in separate file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('selected_columns.txt', 'r') as f:\n",
    "    selected_columns = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a_stopword_pw',\n",
       " 'a_kup_pw',\n",
       " 'a_subtlex_us_zipf_pw',\n",
       " 'a_subtlex_us_zipf_ps',\n",
       " 'simp_adj_var',\n",
       " 'corr_det_var',\n",
       " 'bilog_ttr',\n",
       " 'n_upunct',\n",
       " 'a_adj_pw',\n",
       " 'a_adp_pw',\n",
       " 'a_aux_pw',\n",
       " 'a_det_pw',\n",
       " 'a_num_pw',\n",
       " 'a_punct_pw',\n",
       " 'a_space_pw',\n",
       " 'a_det_ps',\n",
       " 'a_num_ps',\n",
       " 'a_punct_ps',\n",
       " 'fogi',\n",
       " 'cole']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrzejrostkowski/python-code/.venv/lib/python3.9/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0.0 and may not be 100% compatible with the current version (3.7.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text      :)\n",
      "label     CG\n",
      "target     1\n",
      "Name: 2170, dtype: object\n",
      "text      0\n",
      "label     0\n",
      "target    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(amazon_single.iloc[2170])\n",
    "print(amazon_single.isna().sum())\n",
    "amazon_single = amazon_single.drop(2170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     text label  target  length\n",
      "6743    A    OG       0       1\n",
      "8047    a    OG       0       1\n",
      "7018    k    OG       0       1\n",
      "5544    A    OG       0       1\n",
      "9372    a    OG       0       1\n",
      "7962   ok    OG       0       2\n",
      "7406   ok    OG       0       2\n",
      "7554   :)    OG       0       2\n",
      "6960   ty    OG       0       2\n",
      "9612   Ok    OG       0       2\n",
      "9110   OK    OG       0       2\n",
      "6961   Ok    OG       0       2\n",
      "5868   ok    OG       0       2\n",
      "5073   ok    OG       0       2\n",
      "7255   ok    OG       0       2\n",
      "2170   :)    CG       1       2\n",
      "6835   ok    OG       0       2\n",
      "6151   ok    OG       0       2\n",
      "9381   ok    OG       0       2\n",
      "7743   OK    OG       0       2\n"
     ]
    }
   ],
   "source": [
    "amazon_test = amazon_single\n",
    "amazon_test['length'] = amazon_test['text'].apply(len)\n",
    "amazon_single_sorted = amazon_test.sort_values('length')\n",
    "# Zakładam, że 'text' to kolumna, której długość chcemy obliczyć\n",
    "amazon_single['length'] = amazon_single['text'].apply(len)\n",
    "\n",
    "# Sortujemy DataFrame według kolumny 'length'\n",
    "amazon_single_sorted = amazon_single.sort_values('length')\n",
    "\n",
    "# Wyświetlamy najkrótsze rekordy\n",
    "print(amazon_single_sorted.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     text label  target  length\n",
      "12       Amazing product!    CG       1      16\n",
      "16           not worth it    CG       1      12\n",
      "18             Impressed!    CG       1      10\n",
      "20          Loads of fun!    CG       1      13\n",
      "24    Highly recommended!    CG       1      19\n",
      "...                   ...   ...     ...     ...\n",
      "9950                great    OG       0       5\n",
      "9960         Nice product    OG       0      12\n",
      "9965        Works for me.    OG       0      13\n",
      "9979                   OK    OG       0       2\n",
      "9986   Works Great Thanks    OG       0      18\n",
      "\n",
      "[1227 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Wybieramy rekordy, które mają mniej niż 100 znaków\n",
    "short_records = amazon_single.loc[amazon_single['length'] < 20]\n",
    "\n",
    "# Wyświetlamy te rekordy\n",
    "print(short_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8s/bq8p3kcj0n7ckt9bpcm56l8r0000gn/T/ipykernel_25813/1280456597.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_df = df.groupby('target').apply(lambda x: x.sample(n=500)).reset_index(drop=True)\n",
      "/var/folders/8s/bq8p3kcj0n7ckt9bpcm56l8r0000gn/T/ipykernel_25813/1280456597.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_df = df.groupby('target').apply(lambda x: x.sample(n=500)).reset_index(drop=True)\n",
      "/var/folders/8s/bq8p3kcj0n7ckt9bpcm56l8r0000gn/T/ipykernel_25813/1280456597.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_df = df.groupby('target').apply(lambda x: x.sample(n=500)).reset_index(drop=True)\n",
      "/var/folders/8s/bq8p3kcj0n7ckt9bpcm56l8r0000gn/T/ipykernel_25813/1280456597.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_df = df.groupby('target').apply(lambda x: x.sample(n=500)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "#losuje po 1000 obserwacji\n",
    "sampled_dataframes = {}\n",
    "\n",
    "for name in ['amazon_single','yelp_single', 'amazon_ext', 'yelp_ext', 'general_gpt3']:\n",
    "    df = zbiory[name]\n",
    "    sampled_df = df.groupby('target').apply(lambda x: x.sample(n=500)).reset_index(drop=True)\n",
    "    sampled_dataframes[name] = sampled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "target\n",
      "0    500\n",
      "1    500\n",
      "Name: count, dtype: int64\n",
      "1000\n",
      "target\n",
      "0    500\n",
      "1    500\n",
      "Name: count, dtype: int64\n",
      "1000\n",
      "target\n",
      "0    500\n",
      "1    500\n",
      "Name: count, dtype: int64\n",
      "1000\n",
      "target\n",
      "0    500\n",
      "1    500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for name in sampled_dataframes:\n",
    "    print(len(sampled_dataframes[name]))\n",
    "    print(sampled_dataframes[name]['target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataframes['general_gpt4'] = general_gpt4\n",
    "sampled_dataframes['human'] = human\n",
    "sampled_dataframes['llama3'] = llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['amazon_single', 'amazon_ext', 'yelp_ext', 'general_gpt3', 'general_gpt4', 'human', 'llama3'])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_dataframes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET amazon_single...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [16:27<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET amazon_ext...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [26:15<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET yelp_ext...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [37:59<00:00,  2.28s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET general_gpt3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [30:44<00:00,  1.84s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET general_gpt4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [38:05<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET human...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [03:09<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET llama3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [34:51<00:00,  2.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to each row in the 'text' column of the DataFrame\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "dataframes_linguistic = {}\n",
    "\n",
    "for name, df in sampled_dataframes.items():\n",
    "    print(\"=\" * 50)  # Dodajemy linie złożoną z 50 znaków \"=\"\n",
    "    print((f\"DATASET {name}...\"))\n",
    "    try:\n",
    "        data_features = df.join(df['text'].progress_apply(lambda x: extract_features(x, nlp, selected_columns)))\n",
    "        #data_features = df.join(df['text'].apply(extract_features))\n",
    "        data_features.to_pickle(f\"{name}_linguistic.pkl\")\n",
    "        data_features.to_csv(f\"{name}_linguistic.csv\")\n",
    "\n",
    "        dataframes_linguistic[name] = data_features\n",
    "    except Exception as e:\n",
    "        print(f\"Wystąpił błąd podczas przetwarzania zestawu danych {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8s/bq8p3kcj0n7ckt9bpcm56l8r0000gn/T/ipykernel_25813/4034006719.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_yelp_single = yelp_single.groupby('target').apply(lambda x: x.sample(n=500)).reset_index(drop=True)\n",
      "100%|██████████| 1000/1000 [34:01<00:00,  2.04s/it] \n"
     ]
    }
   ],
   "source": [
    "## zapomnialem o yelp single\n",
    "tqdm.pandas()\n",
    "sampled_yelp_single = yelp_single.groupby('target').apply(lambda x: x.sample(n=500)).reset_index(drop=True)\n",
    "linguistic_yelp_single = sampled_yelp_single.join(sampled_yelp_single['text'].progress_apply(lambda x: extract_features(x, nlp, selected_columns)))\n",
    "#data_features = df.join(df['text'].apply(extract_features))\n",
    "linguistic_yelp_single.to_pickle(\"yelp_single_linguistic.pkl\")\n",
    "linguistic_yelp_single.to_csv(\"yelp_single_linguistic.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_linguistic['yelp_single'] = linguistic_yelp_single\n",
    "sampled_dataframes['yelp_single'] = sampled_yelp_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_single\n",
      "Index(['text', 'label', 'target', 'a_stopword_pw', 'a_kup_pw',\n",
      "       'a_subtlex_us_zipf_pw', 'a_subtlex_us_zipf_ps', 'simp_adj_var',\n",
      "       'corr_det_var', 'bilog_ttr', 'n_upunct', 'a_adj_pw', 'a_adp_pw',\n",
      "       'a_aux_pw', 'a_det_pw', 'a_num_pw', 'a_punct_pw', 'a_space_pw',\n",
      "       'a_det_ps', 'a_num_ps', 'a_punct_ps', 'fogi', 'cole'],\n",
      "      dtype='object')\n",
      "amazon_ext\n",
      "Index(['category', 'rating', 'label', 'text', 'target', 'a_stopword_pw',\n",
      "       'a_kup_pw', 'a_subtlex_us_zipf_pw', 'a_subtlex_us_zipf_ps',\n",
      "       'simp_adj_var', 'corr_det_var', 'bilog_ttr', 'n_upunct', 'a_adj_pw',\n",
      "       'a_adp_pw', 'a_aux_pw', 'a_det_pw', 'a_num_pw', 'a_punct_pw',\n",
      "       'a_space_pw', 'a_det_ps', 'a_num_ps', 'a_punct_ps', 'fogi', 'cole'],\n",
      "      dtype='object')\n",
      "yelp_ext\n",
      "Index(['text', 'target', 'a_stopword_pw', 'a_kup_pw', 'a_subtlex_us_zipf_pw',\n",
      "       'a_subtlex_us_zipf_ps', 'simp_adj_var', 'corr_det_var', 'bilog_ttr',\n",
      "       'n_upunct', 'a_adj_pw', 'a_adp_pw', 'a_aux_pw', 'a_det_pw', 'a_num_pw',\n",
      "       'a_punct_pw', 'a_space_pw', 'a_det_ps', 'a_num_ps', 'a_punct_ps',\n",
      "       'fogi', 'cole'],\n",
      "      dtype='object')\n",
      "general_gpt3\n",
      "Index(['text', 'label', 'target', 'a_stopword_pw', 'a_kup_pw',\n",
      "       'a_subtlex_us_zipf_pw', 'a_subtlex_us_zipf_ps', 'simp_adj_var',\n",
      "       'corr_det_var', 'bilog_ttr', 'n_upunct', 'a_adj_pw', 'a_adp_pw',\n",
      "       'a_aux_pw', 'a_det_pw', 'a_num_pw', 'a_punct_pw', 'a_space_pw',\n",
      "       'a_det_ps', 'a_num_ps', 'a_punct_ps', 'fogi', 'cole'],\n",
      "      dtype='object')\n",
      "general_gpt4\n",
      "Index(['text', 'label', 'target', 'a_stopword_pw', 'a_kup_pw',\n",
      "       'a_subtlex_us_zipf_pw', 'a_subtlex_us_zipf_ps', 'simp_adj_var',\n",
      "       'corr_det_var', 'bilog_ttr', 'n_upunct', 'a_adj_pw', 'a_adp_pw',\n",
      "       'a_aux_pw', 'a_det_pw', 'a_num_pw', 'a_punct_pw', 'a_space_pw',\n",
      "       'a_det_ps', 'a_num_ps', 'a_punct_ps', 'fogi', 'cole'],\n",
      "      dtype='object')\n",
      "human\n",
      "Index(['text', 'target', 'a_stopword_pw', 'a_kup_pw', 'a_subtlex_us_zipf_pw',\n",
      "       'a_subtlex_us_zipf_ps', 'simp_adj_var', 'corr_det_var', 'bilog_ttr',\n",
      "       'n_upunct', 'a_adj_pw', 'a_adp_pw', 'a_aux_pw', 'a_det_pw', 'a_num_pw',\n",
      "       'a_punct_pw', 'a_space_pw', 'a_det_ps', 'a_num_ps', 'a_punct_ps',\n",
      "       'fogi', 'cole'],\n",
      "      dtype='object')\n",
      "llama3\n",
      "Index(['text', 'label', 'target', 'a_stopword_pw', 'a_kup_pw',\n",
      "       'a_subtlex_us_zipf_pw', 'a_subtlex_us_zipf_ps', 'simp_adj_var',\n",
      "       'corr_det_var', 'bilog_ttr', 'n_upunct', 'a_adj_pw', 'a_adp_pw',\n",
      "       'a_aux_pw', 'a_det_pw', 'a_num_pw', 'a_punct_pw', 'a_space_pw',\n",
      "       'a_det_ps', 'a_num_ps', 'a_punct_ps', 'fogi', 'cole'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_linguistic.items():\n",
    "    print(name)\n",
    "    print(dataframes_linguistic[name].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dataframes_linguistic.items():\n",
    "    dataframes_linguistic[name] = df.loc[:, selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_single\n",
      "Index(['a_stopword_pw', 'a_kup_pw', 'a_subtlex_us_zipf_pw',\n",
      "       'a_subtlex_us_zipf_ps', 'simp_adj_var', 'corr_det_var', 'bilog_ttr',\n",
      "       'n_upunct', 'a_adj_pw', 'a_adp_pw', 'a_aux_pw', 'a_det_pw', 'a_num_pw',\n",
      "       'a_punct_pw', 'a_space_pw', 'a_det_ps', 'a_num_ps', 'a_punct_ps',\n",
      "       'fogi', 'cole'],\n",
      "      dtype='object')\n",
      "amazon_ext\n",
      "Index(['a_stopword_pw', 'a_kup_pw', 'a_subtlex_us_zipf_pw',\n",
      "       'a_subtlex_us_zipf_ps', 'simp_adj_var', 'corr_det_var', 'bilog_ttr',\n",
      "       'n_upunct', 'a_adj_pw', 'a_adp_pw', 'a_aux_pw', 'a_det_pw', 'a_num_pw',\n",
      "       'a_punct_pw', 'a_space_pw', 'a_det_ps', 'a_num_ps', 'a_punct_ps',\n",
      "       'fogi', 'cole'],\n",
      "      dtype='object')\n",
      "yelp_ext\n",
      "Index(['a_stopword_pw', 'a_kup_pw', 'a_subtlex_us_zipf_pw',\n",
      "       'a_subtlex_us_zipf_ps', 'simp_adj_var', 'corr_det_var', 'bilog_ttr',\n",
      "       'n_upunct', 'a_adj_pw', 'a_adp_pw', 'a_aux_pw', 'a_det_pw', 'a_num_pw',\n",
      "       'a_punct_pw', 'a_space_pw', 'a_det_ps', 'a_num_ps', 'a_punct_ps',\n",
      "       'fogi', 'cole'],\n",
      "      dtype='object')\n",
      "general_gpt3\n",
      "Index(['a_stopword_pw', 'a_kup_pw', 'a_subtlex_us_zipf_pw',\n",
      "       'a_subtlex_us_zipf_ps', 'simp_adj_var', 'corr_det_var', 'bilog_ttr',\n",
      "       'n_upunct', 'a_adj_pw', 'a_adp_pw', 'a_aux_pw', 'a_det_pw', 'a_num_pw',\n",
      "       'a_punct_pw', 'a_space_pw', 'a_det_ps', 'a_num_ps', 'a_punct_ps',\n",
      "       'fogi', 'cole'],\n",
      "      dtype='object')\n",
      "general_gpt4\n",
      "Index(['a_stopword_pw', 'a_kup_pw', 'a_subtlex_us_zipf_pw',\n",
      "       'a_subtlex_us_zipf_ps', 'simp_adj_var', 'corr_det_var', 'bilog_ttr',\n",
      "       'n_upunct', 'a_adj_pw', 'a_adp_pw', 'a_aux_pw', 'a_det_pw', 'a_num_pw',\n",
      "       'a_punct_pw', 'a_space_pw', 'a_det_ps', 'a_num_ps', 'a_punct_ps',\n",
      "       'fogi', 'cole'],\n",
      "      dtype='object')\n",
      "human\n",
      "Index(['a_stopword_pw', 'a_kup_pw', 'a_subtlex_us_zipf_pw',\n",
      "       'a_subtlex_us_zipf_ps', 'simp_adj_var', 'corr_det_var', 'bilog_ttr',\n",
      "       'n_upunct', 'a_adj_pw', 'a_adp_pw', 'a_aux_pw', 'a_det_pw', 'a_num_pw',\n",
      "       'a_punct_pw', 'a_space_pw', 'a_det_ps', 'a_num_ps', 'a_punct_ps',\n",
      "       'fogi', 'cole'],\n",
      "      dtype='object')\n",
      "llama3\n",
      "Index(['a_stopword_pw', 'a_kup_pw', 'a_subtlex_us_zipf_pw',\n",
      "       'a_subtlex_us_zipf_ps', 'simp_adj_var', 'corr_det_var', 'bilog_ttr',\n",
      "       'n_upunct', 'a_adj_pw', 'a_adp_pw', 'a_aux_pw', 'a_det_pw', 'a_num_pw',\n",
      "       'a_punct_pw', 'a_space_pw', 'a_det_ps', 'a_num_ps', 'a_punct_ps',\n",
      "       'fogi', 'cole'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_linguistic.items():\n",
    "    print(name)\n",
    "    print(dataframes_linguistic[name].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amazon_single':                                                   text label  target\n",
       " 0    My second reorder. Have not been disappointed ...    OG       0\n",
       " 1    Good stuff!  Got my herbs looking green and he...    OG       0\n",
       " 2    Crazy Cats. well let me tell you it spells onl...    OG       0\n",
       " 3    This was my first time trying a Larabar. I lov...    OG       0\n",
       " 4                  Wonderful.  I bought 12 more boxes.    OG       0\n",
       " ..                                                 ...   ...     ...\n",
       " 995  I purchased this cat playpen for my two feline...    CG       1\n",
       " 996  My kitchen must-have! I sprinkle this organic ...    CG       1\n",
       " 997  I am amazed by the quality of these Darice B00...    CG       1\n",
       " 998  I was really excited to get the Fiskars Kangar...    CG       1\n",
       " 999  My dog smells amazing after using this shampoo...    CG       1\n",
       " \n",
       " [1000 rows x 3 columns],\n",
       " 'amazon_ext':                          category  rating label  \\\n",
       " 0                  Kindle_Store_5     5.0    OR   \n",
       " 1    Clothing_Shoes_and_Jewelry_5     5.0    OR   \n",
       " 2                 Movies_and_TV_5     2.0    OR   \n",
       " 3                 Movies_and_TV_5     5.0    OR   \n",
       " 4           Sports_and_Outdoors_5     5.0    OR   \n",
       " ..                            ...     ...   ...   \n",
       " 995                Pet_Supplies_5     5.0    CG   \n",
       " 996         Sports_and_Outdoors_5     5.0    CG   \n",
       " 997  Tools_and_Home_Improvement_5     5.0    CG   \n",
       " 998              Toys_and_Games_5     5.0    CG   \n",
       " 999                Kindle_Store_5     4.0    CG   \n",
       " \n",
       "                                                   text  target  \n",
       " 0    For many years now, I've been a fan of the tru...       0  \n",
       " 1    Its really a top quality watch winder. Its mor...       0  \n",
       " 2    This is a perfect example of CGI taking over. ...       0  \n",
       " 3    Grew up watching this series and now I own the...       0  \n",
       " 4    Perfect fit. Will buy more. This only comes wi...       0  \n",
       " ..                                                 ...     ...  \n",
       " 995  My adult older dogs and cats love them.  I've ...       1  \n",
       " 996  They fit like a glove, and the quality is good...       1  \n",
       " 997  Wery good batteries for the price. The only pr...       1  \n",
       " 998  This HK is adorable-- absolutely adorable. The...       1  \n",
       " 999  As usual Nicole Edwards rocks! She's an alpha ...       1  \n",
       " \n",
       " [1000 rows x 5 columns],\n",
       " 'yelp_ext':                                                   text  target\n",
       " 0    I really wished I could fall in love with this...       0\n",
       " 1    Cute little brunch place in the SoHo area. The...       0\n",
       " 2    Pandan Chicken looks beautiful but the chicken...       0\n",
       " 3    Visited Hometown Hotpot & BBQ yesterday for th...       0\n",
       " 4    Please kee0 in mind that this review is in no ...       0\n",
       " ..                                                 ...     ...\n",
       " 995  I had a great experience at Bamboo House! I or...       1\n",
       " 996  The atmosphere at this pizzeria was great - fu...       1\n",
       " 997  I had a lovely dinner at this restaurant befor...       1\n",
       " 998  I had a great time at Jongro KBBQ! The food wa...       1\n",
       " 999  I recently visited this restaurant and was rea...       1\n",
       " \n",
       " [1000 rows x 2 columns],\n",
       " 'general_gpt3':                                                   text label  target\n",
       " 0    I visited the Independent last week for Saturd...    OG       0\n",
       " 1    Tapas are hard to do well.  It either works or...    OG       0\n",
       " 2    Just wasted an hour, they cancelled my appoint...    OG       0\n",
       " 3    Skip fast food and try this place. Not your ty...    OG       0\n",
       " 4    My husband and I received a couples massage fo...    OG       0\n",
       " ..                                                 ...   ...     ...\n",
       " 995  Amazing food, great ambiance, friendly staff, ...    CG       1\n",
       " 996  I recently visited this restaurant and I must ...    CG       1\n",
       " 997  I recently visited this restaurant and I must ...    CG       1\n",
       " 998  I had an amazing experience at this restaurant...    CG       1\n",
       " 999  The restaurant had a cozy ambiance and the ser...    CG       1\n",
       " \n",
       " [1000 rows x 3 columns],\n",
       " 'general_gpt4':                                                   text label  target\n",
       " 0    I recently visited Cafe Belle, and I must say ...    CG       1\n",
       " 1    I recently visited this charming restaurant an...    CG       1\n",
       " 2    Last night, we had the pleasure of dining at T...    CG       1\n",
       " 3    I had the pleasure of dining at La Bella Risto...    CG       1\n",
       " 4    Dined here last night and was extremely disapp...    CG       1\n",
       " ..                                                 ...   ...     ...\n",
       " 995  Zero if I could. So far, I've had to locate a ...    OG       0\n",
       " 996  I love this movie theater and have been coming...    OG       0\n",
       " 997  Swordfish was amazing. Zeppolis with Nutella w...    OG       0\n",
       " 998  Cool atmosphere, nice waitstaff and the burger...    OG       0\n",
       " 999  I've been to the downtown location many times ...    OG       0\n",
       " \n",
       " [1000 rows x 3 columns],\n",
       " 'human':                                                   text  target\n",
       " 0    Great food and great atmosphere! The chicken t...       1\n",
       " 1    I had heard good things about Rose Restaurant ...       1\n",
       " 2    I was driving by rose restaurant one day and d...       1\n",
       " 3    Rose Restaurant had the most modern and up-to-...       1\n",
       " 4    Today is the third time I've come to Gloria Re...       1\n",
       " ..                                                 ...     ...\n",
       " 105  My family and I began dining with Mr. Pal circ...       0\n",
       " 106  Samosa was not tasty at all. It was very plain...       1\n",
       " 107  I'm not sure if the food would be categorized ...       1\n",
       " 108  The restroom of this restraurant is dirty. It ...       1\n",
       " 109  Food was very disappointing and even customer ...       1\n",
       " \n",
       " [110 rows x 2 columns],\n",
       " 'llama3':                                                   text label  target\n",
       " 0    Best corn ever! Taste great and always fresh f...    OG       0\n",
       " 1    \"I'm still trying to process the disaster that...    CG       1\n",
       " 2    \"I recently visited Bistro Bliss and was blown...    CG       1\n",
       " 3    \"I recently visited Bistro Bliss and had a del...    CG       1\n",
       " 4    \"I recently had the pleasure of dining at Bist...    CG       1\n",
       " ..                                                 ...   ...     ...\n",
       " 995  After a heavy lunch at Monell's, my friends an...    OG       0\n",
       " 996  Best milkshakes and burgers in town! Love the ...    OG       0\n",
       " 997  I brought my family to Newks on Saturday 3/7/1...    OG       0\n",
       " 998  This place is the pits.  They always forget so...    OG       0\n",
       " 999  I have had authentic tacos throughout Central ...    OG       0\n",
       " \n",
       " [1000 rows x 3 columns]}"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET amazon_single...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   78.700\n",
      "Precision:   84.915\n",
      "Recall:   69.800\n",
      "F1 score:   76.619\n",
      "(0.787, 0.8491484184914841, 0.698, 0.7661909989023051)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   71.000\n",
      "Precision:   86.458\n",
      "Recall:   49.800\n",
      "F1 score:   63.198\n",
      "(0.71, 0.8645833333333334, 0.498, 0.631979695431472)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   73.700\n",
      "Precision:   88.599\n",
      "Recall:   54.400\n",
      "F1 score:   67.410\n",
      "(0.737, 0.8859934853420195, 0.544, 0.6741016109045849)\n",
      "Evaluating SVM...\n",
      "Accuracy:   78.700\n",
      "Precision:   83.450\n",
      "Recall:   71.600\n",
      "F1 score:   77.072\n",
      "(0.787, 0.8344988344988346, 0.716, 0.7707212055974165)\n",
      "Evaluating MLP...\n",
      "Accuracy:   74.200\n",
      "Precision:   89.032\n",
      "Recall:   55.200\n",
      "F1 score:   68.148\n",
      "(0.742, 0.8903225806451613, 0.552, 0.6814814814814815)\n",
      "==================================================\n",
      "DATASET yelp_single...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   91.300\n",
      "Precision:   89.788\n",
      "Recall:   93.200\n",
      "F1 score:   91.462\n",
      "(0.913, 0.8978805394990366, 0.932, 0.914622178606477)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   84.800\n",
      "Precision:   86.100\n",
      "Recall:   83.000\n",
      "F1 score:   84.521\n",
      "(0.848, 0.8609958506224067, 0.83, 0.845213849287169)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   90.300\n",
      "Precision:   91.207\n",
      "Recall:   89.200\n",
      "F1 score:   90.192\n",
      "(0.903, 0.9120654396728016, 0.892, 0.9019211324570273)\n",
      "Evaluating SVM...\n",
      "Accuracy:   88.400\n",
      "Precision:   85.556\n",
      "Recall:   92.400\n",
      "F1 score:   88.846\n",
      "(0.884, 0.8555555555555555, 0.924, 0.8884615384615384)\n",
      "Evaluating MLP...\n",
      "Accuracy:   91.200\n",
      "Precision:   91.200\n",
      "Recall:   91.200\n",
      "F1 score:   91.200\n",
      "(0.912, 0.912, 0.912, 0.912)\n",
      "==================================================\n",
      "DATASET amazon_ext...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   46.600\n",
      "Precision:   35.345\n",
      "Recall:   8.200\n",
      "F1 score:   13.312\n",
      "(0.466, 0.35344827586206895, 0.082, 0.1331168831168831)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   46.200\n",
      "Precision:   25.641\n",
      "Recall:   4.000\n",
      "F1 score:   6.920\n",
      "(0.462, 0.2564102564102564, 0.04, 0.06920415224913495)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   47.600\n",
      "Precision:   18.421\n",
      "Recall:   1.400\n",
      "F1 score:   2.602\n",
      "(0.476, 0.18421052631578946, 0.014, 0.026022304832713755)\n",
      "Evaluating SVM...\n",
      "Accuracy:   44.100\n",
      "Precision:   28.467\n",
      "Recall:   7.800\n",
      "F1 score:   12.245\n",
      "(0.441, 0.2846715328467153, 0.078, 0.12244897959183673)\n",
      "Evaluating MLP...\n",
      "Accuracy:   47.900\n",
      "Precision:   29.412\n",
      "Recall:   3.000\n",
      "F1 score:   5.445\n",
      "(0.479, 0.29411764705882354, 0.03, 0.0544464609800363)\n",
      "==================================================\n",
      "DATASET yelp_ext...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   63.000\n",
      "Precision:   76.000\n",
      "Recall:   38.000\n",
      "F1 score:   50.667\n",
      "(0.63, 0.76, 0.38, 0.5066666666666667)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   62.200\n",
      "Precision:   70.470\n",
      "Recall:   42.000\n",
      "F1 score:   52.632\n",
      "(0.622, 0.7046979865771812, 0.42, 0.5263157894736842)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   58.900\n",
      "Precision:   75.429\n",
      "Recall:   26.400\n",
      "F1 score:   39.111\n",
      "(0.589, 0.7542857142857143, 0.264, 0.39111111111111113)\n",
      "Evaluating SVM...\n",
      "Accuracy:   62.300\n",
      "Precision:   69.040\n",
      "Recall:   44.600\n",
      "F1 score:   54.192\n",
      "(0.623, 0.6904024767801857, 0.446, 0.5419198055893074)\n",
      "Evaluating MLP...\n",
      "Accuracy:   62.900\n",
      "Precision:   75.097\n",
      "Recall:   38.600\n",
      "F1 score:   50.991\n",
      "(0.629, 0.7509727626459144, 0.386, 0.5099075297225891)\n",
      "==================================================\n",
      "DATASET general_gpt3...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   89.400\n",
      "Precision:   88.031\n",
      "Recall:   91.200\n",
      "F1 score:   89.587\n",
      "(0.894, 0.8803088803088803, 0.912, 0.8958742632612967)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   85.500\n",
      "Precision:   84.872\n",
      "Recall:   86.400\n",
      "F1 score:   85.629\n",
      "(0.855, 0.8487229862475442, 0.864, 0.8562933597621407)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   91.200\n",
      "Precision:   92.917\n",
      "Recall:   89.200\n",
      "F1 score:   91.020\n",
      "(0.912, 0.9291666666666667, 0.892, 0.9102040816326531)\n",
      "Evaluating SVM...\n",
      "Accuracy:   86.500\n",
      "Precision:   85.164\n",
      "Recall:   88.400\n",
      "F1 score:   86.752\n",
      "(0.865, 0.8516377649325626, 0.884, 0.8675171736997056)\n",
      "Evaluating MLP...\n",
      "Accuracy:   89.500\n",
      "Precision:   90.061\n",
      "Recall:   88.800\n",
      "F1 score:   89.426\n",
      "(0.895, 0.9006085192697769, 0.888, 0.8942598187311178)\n",
      "==================================================\n",
      "DATASET general_gpt4...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   91.800\n",
      "Precision:   90.057\n",
      "Recall:   94.012\n",
      "F1 score:   91.992\n",
      "(0.918, 0.9005736137667304, 0.9401197604790419, 0.919921875)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   88.700\n",
      "Precision:   87.165\n",
      "Recall:   90.818\n",
      "F1 score:   88.954\n",
      "(0.887, 0.8716475095785441, 0.908183632734531, 0.8895405669599218)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   93.700\n",
      "Precision:   93.625\n",
      "Recall:   93.812\n",
      "F1 score:   93.719\n",
      "(0.937, 0.9362549800796812, 0.93812375249501, 0.9371884346959123)\n",
      "Evaluating SVM...\n",
      "Accuracy:   89.800\n",
      "Precision:   87.013\n",
      "Recall:   93.613\n",
      "F1 score:   90.192\n",
      "(0.898, 0.8701298701298701, 0.936127744510978, 0.9019230769230769)\n",
      "Evaluating MLP...\n",
      "Accuracy:   91.200\n",
      "Precision:   91.549\n",
      "Recall:   90.818\n",
      "F1 score:   91.182\n",
      "(0.912, 0.9154929577464789, 0.908183632734531, 0.9118236472945892)\n",
      "==================================================\n",
      "DATASET human...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   50.909\n",
      "Precision:   55.556\n",
      "Recall:   9.091\n",
      "F1 score:   15.625\n",
      "(0.509090909090909, 0.5555555555555556, 0.09090909090909091, 0.15625)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   49.091\n",
      "Precision:   46.154\n",
      "Recall:   10.909\n",
      "F1 score:   17.647\n",
      "(0.4909090909090909, 0.46153846153846156, 0.10909090909090909, 0.17647058823529413)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   46.364\n",
      "Precision:   25.000\n",
      "Recall:   3.636\n",
      "F1 score:   6.349\n",
      "(0.4636363636363636, 0.25, 0.03636363636363636, 0.06349206349206349)\n",
      "Evaluating SVM...\n",
      "Accuracy:   50.000\n",
      "Precision:   50.000\n",
      "Recall:   10.909\n",
      "F1 score:   17.910\n",
      "(0.5, 0.5, 0.10909090909090909, 0.1791044776119403)\n",
      "Evaluating MLP...\n",
      "Accuracy:   50.000\n",
      "Precision:   50.000\n",
      "Recall:   9.091\n",
      "F1 score:   15.385\n",
      "(0.5, 0.5, 0.09090909090909091, 0.15384615384615385)\n",
      "==================================================\n",
      "DATASET llama3...\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy:   88.100\n",
      "Precision:   88.776\n",
      "Recall:   87.174\n",
      "F1 score:   87.968\n",
      "(0.881, 0.8877551020408163, 0.8717434869739479, 0.8796764408493428)\n",
      "Evaluating K-Nearest Neighbors...\n",
      "Accuracy:   83.800\n",
      "Precision:   86.081\n",
      "Recall:   80.561\n",
      "F1 score:   83.230\n",
      "(0.838, 0.860813704496788, 0.8056112224448898, 0.8322981366459627)\n",
      "Evaluating Random Forest...\n",
      "Accuracy:   86.500\n",
      "Precision:   91.935\n",
      "Recall:   79.960\n",
      "F1 score:   85.531\n",
      "(0.865, 0.9193548387096774, 0.7995991983967936, 0.8553054662379421)\n",
      "Evaluating SVM...\n",
      "Accuracy:   84.900\n",
      "Precision:   84.524\n",
      "Recall:   85.371\n",
      "F1 score:   84.945\n",
      "(0.849, 0.8452380952380952, 0.8537074148296593, 0.8494516450648056)\n",
      "Evaluating MLP...\n",
      "Accuracy:   87.800\n",
      "Precision:   91.796\n",
      "Recall:   82.966\n",
      "F1 score:   87.158\n",
      "(0.878, 0.917960088691796, 0.8296593186372746, 0.871578947368421)\n"
     ]
    }
   ],
   "source": [
    "# Dodajemy nową kolumnę 'F1 Score Spacy' z domyślną wartością NaN\n",
    "results['Accuracy Linguistic'] = np.nan\n",
    "\n",
    "for name, df in zbiory.items():\n",
    "    print(\"=\" * 50)  # Dodajemy linie złożoną z 50 znaków \"=\"\n",
    "    print((f\"DATASET {name}...\"))\n",
    "    for model in models_spacy:\n",
    "        print(f\"Evaluating {model}...\")\n",
    "        metrics = evaluate_model(loaded_models_linguistic[model], dataframes_linguistic[name], sampled_dataframes[name][\"target\"], cm=False)\n",
    "        print(metrics)\n",
    "        # Dodajemy wyniki do nowej kolumny 'Accuracy Spacy'\n",
    "        results.loc[(results['Dataset'] == name) & (results['Model'] == model), 'Accuracy Linguistic'] = metrics[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy TFIDF</th>\n",
       "      <th>Accuracy Spacy</th>\n",
       "      <th>Accuracy Linguistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.727096</td>\n",
       "      <td>0.764974</td>\n",
       "      <td>0.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.763263</td>\n",
       "      <td>0.698460</td>\n",
       "      <td>0.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.698460</td>\n",
       "      <td>0.706902</td>\n",
       "      <td>0.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.797376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.758357</td>\n",
       "      <td>0.762122</td>\n",
       "      <td>0.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>amazon_single</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.797718</td>\n",
       "      <td>0.746720</td>\n",
       "      <td>0.742000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.979700</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>0.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.841400</td>\n",
       "      <td>0.899500</td>\n",
       "      <td>0.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.969200</td>\n",
       "      <td>0.909300</td>\n",
       "      <td>0.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.973000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.981100</td>\n",
       "      <td>0.923700</td>\n",
       "      <td>0.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>yelp_single</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.978000</td>\n",
       "      <td>0.935700</td>\n",
       "      <td>0.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.500198</td>\n",
       "      <td>0.483627</td>\n",
       "      <td>0.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.504773</td>\n",
       "      <td>0.488301</td>\n",
       "      <td>0.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.500223</td>\n",
       "      <td>0.494930</td>\n",
       "      <td>0.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.500173</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.499011</td>\n",
       "      <td>0.482390</td>\n",
       "      <td>0.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>amazon_ext</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.498912</td>\n",
       "      <td>0.484962</td>\n",
       "      <td>0.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.585975</td>\n",
       "      <td>0.624606</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.677934</td>\n",
       "      <td>0.604871</td>\n",
       "      <td>0.622000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.598782</td>\n",
       "      <td>0.601512</td>\n",
       "      <td>0.589000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.587655</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.576527</td>\n",
       "      <td>0.626706</td>\n",
       "      <td>0.623000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>yelp_ext</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.578417</td>\n",
       "      <td>0.604031</td>\n",
       "      <td>0.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.991000</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.905500</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>0.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>0.935500</td>\n",
       "      <td>0.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.988500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.987000</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>general_gpt3</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.980500</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.978000</td>\n",
       "      <td>0.927000</td>\n",
       "      <td>0.918000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.889000</td>\n",
       "      <td>0.902000</td>\n",
       "      <td>0.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.923000</td>\n",
       "      <td>0.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.968000</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>0.898000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>general_gpt4</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.961000</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>human</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.554545</td>\n",
       "      <td>0.509091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>human</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.490909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>human</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.463636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>human</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>human</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.554545</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>human</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.518182</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>llama3</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.881000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>llama3</td>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.874000</td>\n",
       "      <td>0.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>llama3</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.903000</td>\n",
       "      <td>0.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>llama3</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>llama3</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.984000</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>0.849000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>llama3</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.878000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Dataset                    Model  Accuracy TFIDF  Accuracy Spacy  \\\n",
       "0   amazon_single      Logistic Regression        0.727096        0.764974   \n",
       "1   amazon_single      K-Nearest Neighbors        0.763263        0.698460   \n",
       "2   amazon_single            Random Forest        0.698460        0.706902   \n",
       "3   amazon_single  Multinomial Naive Bayes        0.797376             NaN   \n",
       "4   amazon_single                      SVM        0.758357        0.762122   \n",
       "5   amazon_single                      MLP        0.797718        0.746720   \n",
       "6     yelp_single      Logistic Regression        0.979700        0.923900   \n",
       "7     yelp_single      K-Nearest Neighbors        0.841400        0.899500   \n",
       "8     yelp_single            Random Forest        0.969200        0.909300   \n",
       "9     yelp_single  Multinomial Naive Bayes        0.973000             NaN   \n",
       "10    yelp_single                      SVM        0.981100        0.923700   \n",
       "11    yelp_single                      MLP        0.978000        0.935700   \n",
       "12     amazon_ext      Logistic Regression        0.500198        0.483627   \n",
       "13     amazon_ext      K-Nearest Neighbors        0.504773        0.488301   \n",
       "14     amazon_ext            Random Forest        0.500223        0.494930   \n",
       "15     amazon_ext  Multinomial Naive Bayes        0.500173             NaN   \n",
       "16     amazon_ext                      SVM        0.499011        0.482390   \n",
       "17     amazon_ext                      MLP        0.498912        0.484962   \n",
       "18       yelp_ext      Logistic Regression        0.585975        0.624606   \n",
       "19       yelp_ext      K-Nearest Neighbors        0.677934        0.604871   \n",
       "20       yelp_ext            Random Forest        0.598782        0.601512   \n",
       "21       yelp_ext  Multinomial Naive Bayes        0.587655             NaN   \n",
       "22       yelp_ext                      SVM        0.576527        0.626706   \n",
       "23       yelp_ext                      MLP        0.578417        0.604031   \n",
       "24   general_gpt3      Logistic Regression        0.991000        0.922500   \n",
       "25   general_gpt3      K-Nearest Neighbors        0.905500        0.918000   \n",
       "26   general_gpt3            Random Forest        0.987500        0.935500   \n",
       "27   general_gpt3  Multinomial Naive Bayes        0.988500             NaN   \n",
       "28   general_gpt3                      SVM        0.987000        0.917500   \n",
       "29   general_gpt3                      MLP        0.980500        0.937500   \n",
       "30   general_gpt4      Logistic Regression        0.978000        0.927000   \n",
       "31   general_gpt4      K-Nearest Neighbors        0.889000        0.902000   \n",
       "32   general_gpt4            Random Forest        0.980000        0.923000   \n",
       "33   general_gpt4  Multinomial Naive Bayes        0.975000             NaN   \n",
       "34   general_gpt4                      SVM        0.968000        0.922000   \n",
       "35   general_gpt4                      MLP        0.961000        0.926000   \n",
       "36          human      Logistic Regression        0.509091        0.554545   \n",
       "37          human      K-Nearest Neighbors        0.490909        0.527273   \n",
       "38          human            Random Forest        0.490909        0.481818   \n",
       "39          human  Multinomial Naive Bayes        0.500000             NaN   \n",
       "40          human                      SVM        0.509091        0.554545   \n",
       "41          human                      MLP        0.500000        0.518182   \n",
       "42         llama3      Logistic Regression        0.985000        0.924000   \n",
       "43         llama3      K-Nearest Neighbors        0.894000        0.874000   \n",
       "44         llama3            Random Forest        0.980000        0.903000   \n",
       "45         llama3  Multinomial Naive Bayes        0.981000             NaN   \n",
       "46         llama3                      SVM        0.984000        0.929000   \n",
       "47         llama3                      MLP        0.981000        0.940000   \n",
       "\n",
       "    Accuracy Linguistic  \n",
       "0              0.787000  \n",
       "1              0.710000  \n",
       "2              0.737000  \n",
       "3                   NaN  \n",
       "4              0.787000  \n",
       "5              0.742000  \n",
       "6              0.913000  \n",
       "7              0.848000  \n",
       "8              0.903000  \n",
       "9                   NaN  \n",
       "10             0.884000  \n",
       "11             0.912000  \n",
       "12             0.466000  \n",
       "13             0.462000  \n",
       "14             0.476000  \n",
       "15                  NaN  \n",
       "16             0.441000  \n",
       "17             0.479000  \n",
       "18             0.630000  \n",
       "19             0.622000  \n",
       "20             0.589000  \n",
       "21                  NaN  \n",
       "22             0.623000  \n",
       "23             0.629000  \n",
       "24             0.894000  \n",
       "25             0.855000  \n",
       "26             0.912000  \n",
       "27                  NaN  \n",
       "28             0.865000  \n",
       "29             0.895000  \n",
       "30             0.918000  \n",
       "31             0.887000  \n",
       "32             0.937000  \n",
       "33                  NaN  \n",
       "34             0.898000  \n",
       "35             0.912000  \n",
       "36             0.509091  \n",
       "37             0.490909  \n",
       "38             0.463636  \n",
       "39                  NaN  \n",
       "40             0.500000  \n",
       "41             0.500000  \n",
       "42             0.881000  \n",
       "43             0.838000  \n",
       "44             0.865000  \n",
       "45                  NaN  \n",
       "46             0.849000  \n",
       "47             0.878000  "
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
